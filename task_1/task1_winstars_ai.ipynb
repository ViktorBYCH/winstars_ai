{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7c7da9-2219-43e3-b247-7fe1949017a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input one algorithm as sample: cnn, nn, rf cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 198\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Выбираем модель\u001b[39;00m\n\u001b[0;32m    197\u001b[0m classifier \u001b[38;5;241m=\u001b[39m MnistClassifier(algorithm\u001b[38;5;241m=\u001b[39muse_algorithm)  \u001b[38;5;66;03m# Выбираем: \"cnn\", \"nn\", \"rf\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m classifier\u001b[38;5;241m.\u001b[39mtrain_model(train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# Прогнозирование\u001b[39;00m\n\u001b[0;32m    201\u001b[0m sample_data, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_loader))\n",
      "Cell \u001b[1;32mIn[3], line 181\u001b[0m, in \u001b[0;36mMnistClassifier.train_model\u001b[1;34m(self, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_model(train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39mepochs)\n",
      "Cell \u001b[1;32mIn[3], line 115\u001b[0m, in \u001b[0;36mCNN.train_model\u001b[1;34m(self, train_loader, val_loader, epochs, save_path)\u001b[0m\n\u001b[0;32m    113\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(images)\n\u001b[0;32m    114\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m--> 115\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    116\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    117\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from abc import ABC, abstractmethod  # Abstract base class and method\n",
    "import torch  # PyTorch for deep learning models\n",
    "import torch.nn as nn  # Neural network modules from PyTorch\n",
    "import torch.optim as optim  # Optimizers for training models\n",
    "import torch.nn.functional as F  # Functions for layers and activations\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest classifier from sklearn\n",
    "import joblib  # For saving and loading models\n",
    "import torchvision  # For MNIST dataset and other torchvision models\n",
    "import torchvision.transforms as transforms  # For data transformations\n",
    "from torch.utils.data import DataLoader  # For batching data\n",
    "from tqdm import tqdm  # Progress bar for loops\n",
    "import matplotlib.pyplot as plt  # For visualizations\n",
    "\n",
    "# Taking user input for the algorithm selection\n",
    "use_algorithm = input(str('Input one algorithm as sample: cnn, nn, rf'))\n",
    "\n",
    "# Interface class for all models, ensuring that models implement `train_model` and `predict`\n",
    "class MnistClassifierInterface:\n",
    "    @abstractmethod\n",
    "    def train_model(self, train_loader, val_loader, epochs=10):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# --- Feed-Forward Neural Network Class ---\n",
    "class FeedForwardNN(nn.Module, MnistClassifierInterface):\n",
    "    def __init__(self, input_size=28*28, hidden_size=128, output_size=10):\n",
    "        # Initialize the neural network with input, hidden and output sizes\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),  # Input layer\n",
    "            nn.ReLU(),  # ReLU activation\n",
    "            nn.Linear(hidden_size, output_size)  # Output layer (10 classes for MNIST)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, epochs=10, save_path=\"ffnn_best.pth\"):\n",
    "        # Method to train the feed-forward neural network\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)  # Adam optimizer\n",
    "        criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "        best_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for images, labels in train_loader:\n",
    "                images = images.view(images.size(0), -1)  # Flatten images\n",
    "                optimizer.zero_grad()  # Zero out gradients\n",
    "                outputs = self(images)  # Get outputs\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                loss.backward()  # Backpropagate gradients\n",
    "                optimizer.step()  # Update weights\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            val_loss = self.evaluate(val_loader, criterion)  # Validation loss\n",
    "            print(f\"Epoch {epoch+1}: Train Loss={total_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "\n",
    "            # Save model if validation loss improves\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save(self.state_dict(), save_path)\n",
    "                print(f\"Saved best model parameters - Epoch {epoch+1}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    def evaluate(self, loader, criterion):\n",
    "        # Method to evaluate model on validation set\n",
    "        self.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():  # No gradient tracking in evaluation\n",
    "            for images, labels in loader:\n",
    "                images = images.view(images.size(0), -1)\n",
    "                outputs = self(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(loader)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Method to make predictions on new data\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            X = X.view(X.size(0), -1)\n",
    "            outputs = self(X)\n",
    "            return torch.argmax(outputs, dim=1)\n",
    "\n",
    "\n",
    "# --- Convolutional Neural Network Class ---\n",
    "class CNN(nn.Module, MnistClassifierInterface):\n",
    "    def __init__(self):\n",
    "        # Initialize CNN with multiple convolutional and pooling layers\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # First convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # Max pooling layer\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # Second convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),  # Flatten the image for the fully connected layer\n",
    "            nn.Linear(32 * 7 * 7, 128),  # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)  # Output layer (10 classes for MNIST)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, epochs=10, save_path=\"cnn_best.pth\"):\n",
    "        # Method to train the CNN model\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        best_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for images, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            val_loss = self.evaluate(val_loader, criterion)\n",
    "            print(f\"Epoch {epoch+1}: Train Loss={total_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save(self.state_dict(), save_path)\n",
    "                print(f\"Saved best model parameters - Epoch {epoch+1}, Val Loss: {val_loss:.4f}\")\n",
    "                \n",
    "    def evaluate(self, loader, criterion):\n",
    "        # Method to evaluate the CNN model on validation set\n",
    "        self.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                outputs = self(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(loader)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Method to make predictions using CNN\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "            return torch.argmax(outputs, dim=1)\n",
    "\n",
    "\n",
    "# --- Random Forest Class for MNIST ---\n",
    "class RandomForestMnist(MnistClassifierInterface):\n",
    "    def __init__(self):\n",
    "        # Initialize Random Forest model from sklearn\n",
    "        self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader=None, epochs=10, save_path=\"rf_best.pkl\"):\n",
    "        # Train the Random Forest model\n",
    "        X_train, y_train = self._prepare_data(train_loader)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        joblib.dump(self.model, save_path)\n",
    "\n",
    "    def _prepare_data(self, loader):\n",
    "        # Convert images and labels into a format suitable for Random Forest\n",
    "        X, y = [], []\n",
    "        for images, labels in loader:\n",
    "            images = images.view(images.size(0), -1).numpy()\n",
    "            X.extend(images)\n",
    "            y.extend(labels.numpy())\n",
    "        return X, y\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using Random Forest\n",
    "        X = X.view(X.size(0), -1).numpy()\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "# --- Class to manage training process and select models ---\n",
    "class MnistClassifier:\n",
    "    def __init__(self, algorithm):\n",
    "        # Select the model based on user input\n",
    "        self.algorithm = algorithm\n",
    "        if algorithm == \"cnn\":\n",
    "            self.model = CNN()\n",
    "        elif algorithm == \"nn\":\n",
    "            self.model = FeedForwardNN()\n",
    "        elif algorithm == \"rf\":\n",
    "            self.model = RandomForestMnist()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported algorithm. Choose from ['cnn', 'nn', 'rf']\")\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, epochs=10):\n",
    "        # Train the selected model\n",
    "        self.model.train_model(train_loader, val_loader, epochs=epochs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using the selected model\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "# --- Load MNIST Dataset ---\n",
    "transform = transforms.Compose([transforms.ToTensor()])  # Transform to tensor\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "val_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# DataLoader for batching the dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Select the model based on user input\n",
    "classifier = MnistClassifier(algorithm=use_algorithm)\n",
    "classifier.train_model(train_loader, val_loader, epochs=10)\n",
    "\n",
    "# Make predictions\n",
    "sample_data, _ = next(iter(val_loader))\n",
    "predictions = classifier.predict(sample_data)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "\n",
    "# Function to prepare and display an image sample\n",
    "def show_sample_image(inpt):\n",
    "    # Load the trained model based on the input algorithm type\n",
    "    if inpt == \"rf\":  # Random Forest model\n",
    "        classifier = MnistClassifier(algorithm=\"rf\")  # Instantiate the RandomForest classifier\n",
    "        classifier.model = joblib.load(\"rf_best.pkl\")  # Load the pre-trained Random Forest model\n",
    "\n",
    "    else:\n",
    "        if inpt == \"nn\":  # Feed-Forward Neural Network model\n",
    "            classifier = MnistClassifier(algorithm=\"nn\")  # Instantiate the Feed-Forward Neural Network classifier\n",
    "            classifier.model.load_state_dict(torch.load(\"ffnn_best.pth\"))  # Load the pre-trained NN model weights\n",
    "        else:  # Convolutional Neural Network model\n",
    "            classifier = MnistClassifier(algorithm=\"cnn\")  # Instantiate the CNN classifier\n",
    "            classifier.model.load_state_dict(torch.load(\"cnn_best.pth\"))  # Load the pre-trained CNN model weights\n",
    "    \n",
    "    # Load the test dataset (MNIST)\n",
    "    transform = transforms.Compose([transforms.ToTensor()])  # Transform the images to tensors\n",
    "    test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)  # Load the MNIST test dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)  # Create a DataLoader for the test set with batch size 1\n",
    "    \n",
    "    # Get the first image and its label from the test dataset\n",
    "    sample_images, sample_labels = next(iter(test_loader))  # Extract the first batch of images and labels\n",
    "    image = sample_images[0].squeeze()  # Remove extra dimensions to display the image properly\n",
    "    label = sample_labels[0].item()  # Get the label (true class) of the first image\n",
    "\n",
    "    # Prepare the image for prediction (convert to the appropriate format based on model type)\n",
    "    if inpt == \"rf\":  # For Random Forest, convert the image into a 1D vector (flatten the 28x28 image into a 784-length vector)\n",
    "        img = sample_images[0].view(-1).numpy().reshape(1, -1)  # Flatten and convert to numpy array for Random Forest\n",
    "    else:  # For NN and CNN, add batch dimension to the image (already in tensor form)\n",
    "        img = sample_images[0].unsqueeze(0)  # Add batch dimension for the neural networks\n",
    "\n",
    "    # Perform prediction using the classifier's abstracted prediction method\n",
    "    prediction = classifier.predict(img)  # Make the prediction\n",
    "    print(f\"Predicted class: {prediction.item()}\")  # Print the predicted class\n",
    "\n",
    "    # Visualize the sample image\n",
    "    plt.imshow(image, cmap=\"gray\")  # Display the image in grayscale\n",
    "    plt.title(f\"Label: {label}, Predicted: {prediction.item()}\")  # Display the true label and predicted label in the title\n",
    "    plt.axis('off')  # Hide the axis to focus on the image\n",
    "    plt.show()  # Show the image with the title\n",
    "\n",
    "# Main block to visualize the image sample based on the chosen model\n",
    "if __name__ == \"__main__\":\n",
    "    show_sample_image(use_algorithm)  # Call the function with the selected algorithm (cnn, nn, or rf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
