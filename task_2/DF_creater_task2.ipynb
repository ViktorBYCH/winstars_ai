{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e452908d-e5a8-47d0-a1bb-49528a408786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m info  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglossary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explain  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\cli\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Needed for testing\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download \u001b[38;5;28;01mas\u001b[39;00m download_module  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m app, setup_cli  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\cli\\download.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Sequence\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urljoin\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyper\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\__init__.py:164\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m packages, utils\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__version__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    153\u001b[0m     __author__,\n\u001b[0;32m    154\u001b[0m     __author_email__,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m     __version__,\n\u001b[0;32m    163\u001b[0m )\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delete, get, head, options, patch, post, put, request\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;167;01mConnectionError\u001b[39;00m,\n\u001b[0;32m    167\u001b[0m     ConnectTimeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m     URLRequired,\n\u001b[0;32m    176\u001b[0m )\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreparedRequest, Request, Response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mrequests.api\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m:license: Apache2, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sessions\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs and sends a :class:`Request <Request>`.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m      <Response [200]>\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timedelta\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_native_string\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPAdapter\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _basic_auth_str\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mapping, cookielib, urljoin, urlparse\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:81\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mssl\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     _preloaded_ssl_context \u001b[38;5;241m=\u001b[39m create_urllib3_context()\n\u001b[1;32m---> 81\u001b[0m     _preloaded_ssl_context\u001b[38;5;241m.\u001b[39mload_verify_locations(\n\u001b[0;32m     82\u001b[0m         extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)\n\u001b[0;32m     83\u001b[0m     )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Bypass default SSLContext creation when Python\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# interpreter isn't built with the ssl module.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     _preloaded_ssl_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Load dataset from a .csv file\n",
    "data = pd.read_csv(\"animals_sentences_full.csv\")\n",
    "\n",
    "# Convert the dataset into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['text']]  # Keep only the 'text' column\n",
    "\n",
    "# Define a set of animal classes to identify in the text\n",
    "animal_classes = {'beaver', 'dolphin', 'otter', 'seal', 'fox', 'spider', 'elephant', 'bear', 'rabbit', 'tiger'}\n",
    "\n",
    "# Initialize the PhraseMatcher for custom entity detection\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Lemmatize animal names to improve matching\n",
    "patterns = []\n",
    "for animal in animal_classes:\n",
    "    doc = nlp(animal.lower())  # Create a Doc object and lemmatize the animal name\n",
    "    lemmas = [token.lemma_ for token in doc]  # Extract lemmas from tokens\n",
    "    pattern = \" \".join(lemmas)  # Join lemmas into a string\n",
    "    patterns.append(nlp.make_doc(pattern))  # Convert into a SpaCy Doc object\n",
    "\n",
    "# Add the lemmatized animal phrases to PhraseMatcher\n",
    "matcher.add(\"ANIMALS\", None, *patterns)\n",
    "\n",
    "# Function to extract and label animal entities in the text\n",
    "def extract_and_label_entities(text):\n",
    "    global x\n",
    "    doc = nlp(text.lower())  # Convert text to lowercase for better lemmatization\n",
    "    \n",
    "    labels = []  # List to store entity labels\n",
    "    \n",
    "    matches = matcher(doc)  # Apply PhraseMatcher\n",
    "       \n",
    "    # Iterate over all tokens and assign entity labels\n",
    "    for i, token in enumerate(doc):\n",
    "        matched = False\n",
    "        for match_id, start, end in matches:\n",
    "            if start <= i < end:\n",
    "                if i == start:\n",
    "                    labels.append((token.text, \"B-ANIMAL\"))  # Beginning of an entity\n",
    "                else:\n",
    "                    labels.append((token.text, \"I-ANIMAL\"))  # Inside an entity\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            labels.append((token.text, \"O\"))  # Not an entity\n",
    "    print(x)\n",
    "    \n",
    "    # Convert labels to a format suitable for model training\n",
    "    labeled_entities = [(token, label) for token, label in labels]\n",
    "    \n",
    "    return labeled_entities\n",
    "\n",
    "print(len(df[\"text\"]))\n",
    "\n",
    "# Apply the entity extraction function to the text column\n",
    "df[\"entities\"] = df[\"text\"].apply(extract_and_label_entities)\n",
    "\n",
    "# Save the annotated data to a CSV file\n",
    "df.to_csv(\"annotated_animals_with_labels_fix.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Annotation completed! File saved as 'annotated_animals_with_labels_fix.csv'.\")\n",
    "\n",
    "# Function to tokenize text and assign corresponding entity labels\n",
    "def create_tokenized_data(text, entities):\n",
    "    words = text.split()  # Split the text into words\n",
    "    labels = ['O'] * len(words)  # Initialize all labels as 'O' (non-entity)\n",
    "    \n",
    "    # Assign B/I entity labels\n",
    "    for entity in entities:\n",
    "        word, label = entity\n",
    "        if word in words:\n",
    "            word_idx = words.index(word)\n",
    "            labels[word_idx] = label\n",
    "    \n",
    "    return {'tokens': words, 'labels': labels}\n",
    "\n",
    "# Apply tokenization and labeling to each row\n",
    "train_data = [create_tokenized_data(row['text'], row['entities']) for _, row in df.iterrows()]\n",
    "\n",
    "# Convert the processed data into a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(train_data)\n",
    "\n",
    "# Convert dataset into a DataFrame for verification\n",
    "df_from_dataset = pd.DataFrame(dataset)\n",
    "\n",
    "# Save the dataset as a CSV file\n",
    "df_from_dataset.to_csv('./dataset.csv', index=False)\n",
    "\n",
    "# Display the first entry of the dataset\n",
    "dataset[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract all labels from the dataset\n",
    "all_labels = []\n",
    "for i in range(len(dataset)):\n",
    "    all_labels.extend(dataset[i]['labels'])\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = set(all_labels)\n",
    "\n",
    "# Print unique labels and their count\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "print(\"Number of unique labels:\", len(unique_labels))\n",
    "\n",
    "# Check if the number of unique labels is exactly 3\n",
    "if len(unique_labels) == 3:\n",
    "    print(\"You have exactly 3 unique labels.\")\n",
    "else:\n",
    "    print(\"Warning! The number of unique labels is not 3.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
